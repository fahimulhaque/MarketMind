services:
  postgres:
    image: pgvector/pgvector:pg16
    environment:
      POSTGRES_DB: ${POSTGRES_DB:-tickeragent}
      POSTGRES_USER: ${POSTGRES_USER:-tickeragent}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-tickeragent}
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-tickeragent} -d ${POSTGRES_DB:-tickeragent}"]
      interval: 10s
      timeout: 5s
      retries: 5
    profiles: ["core", "intelligence", "platform", "full"]

  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      OLLAMA_NUM_PARALLEL: "${OLLAMA_NUM_PARALLEL:-2}"
      OLLAMA_MAX_LOADED_MODELS: "${OLLAMA_MAX_LOADED_MODELS:-1}"
      OLLAMA_KEEP_ALIVE: "${OLLAMA_KEEP_ALIVE:-10m}"
    deploy:
      resources:
        limits:
          cpus: "${OLLAMA_CPU_LIMIT:-4.0}"
          memory: "${OLLAMA_MEM_LIMIT:-4g}"
    healthcheck:
      test: ["CMD-SHELL", "ollama list || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 30
      start_period: 10s
    profiles: ["core", "intelligence", "platform", "full"]

  ollama-init:
    image: ollama/ollama:latest
    depends_on:
      ollama:
        condition: service_healthy
    environment:
      OLLAMA_HOST: http://ollama:11434
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        echo "Pulling LLM model ${OLLAMA_GENERATE_MODEL:-qwen2.5:1.5b}..."
        ollama pull ${OLLAMA_GENERATE_MODEL:-qwen2.5:1.5b}
        echo "Pulling embedding model ${OLLAMA_EMBED_MODEL:-nomic-embed-text}..."
        ollama pull ${OLLAMA_EMBED_MODEL:-nomic-embed-text}
        echo "All models ready."
    profiles: ["core", "intelligence", "platform", "full"]

  api:
    build:
      context: .
      dockerfile: docker/Dockerfile.app
    env_file:
      - .env.prod
    environment:
      POSTGRES_HOST: postgres
      OLLAMA_HOST: http://ollama:11434
      # REDIS is external in prod env, configuring it via .env.prod (e.g. REDIS_HOST=host.docker.internal)
    ports:
      - "8080:8000"
    command: uvicorn api.main:app --host 0.0.0.0 --port 8000
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8000/health', timeout=3)"]
      interval: 15s
      timeout: 5s
      retries: 5
    profiles: ["core", "intelligence", "platform", "full"]

  worker:
    build:
      context: .
      dockerfile: docker/Dockerfile.app
    env_file:
      - .env.prod
    environment:
      POSTGRES_HOST: postgres
      OLLAMA_HOST: http://ollama:11434
    command: celery -A workers.celery_app.celery_app worker --loglevel=INFO
    depends_on:
      postgres:
        condition: service_healthy
    profiles: ["core", "intelligence", "platform", "full"]

  dashboard:
    build:
      context: .
      dockerfile: docker/Dockerfile.dashboard
      args:
        NEXT_PUBLIC_API_BASE: https://api.fahimulhaque.org/tickeragent
        NEXT_PUBLIC_API_WRITE_KEY: ${API_WRITE_KEY:-tickeragent-dev-key}
    environment:
      # Adjusted API Base to proxy through the context /tickeragent
      NEXT_PUBLIC_API_BASE: https://api.fahimulhaque.org/tickeragent
      NEXT_PUBLIC_API_WRITE_KEY: ${API_WRITE_KEY:-tickeragent-dev-key}
    ports:
      - "3005:3000"
    depends_on:
      api:
        condition: service_healthy
    profiles: ["platform", "full"]

volumes:
  postgres_data:
  ollama_data:
